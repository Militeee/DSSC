---
title: "pi_charts"
author: "Salvatore Milite"
date: "30 March 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
res <- read_table2("/home/salvatore/university/DSSC/Lab/Day2/res.txt", 
    col_types = cols(X4 = col_skip()), comment = "#", col_names = FALSE)
res <- apply(res, 2, strsplit, "=")
data <- data.frame(1:18,1:18)
data[1] <- sapply(res[[1]], function(x)x[1])
data[2] <- sapply(res[[3]], function(x)x[2])
names(data) <- c("Implementation","Time")
data[2] <- sapply(data[2], strtrim, 6)
data[2] <- sapply(data[2], as.numeric)

```

## Plots for paralle pi calculation
We can ssee how the problem scales well with the number of processor (except for 20 processors where, for instance, memory bandwidth saturation can lead to not so-good results), nevertheless my implementation uses a auxiliary structure for storing the per-threads partial value of the summationa, in that way the number of accesses to the global variable is equal to the number of processors. In this setting, given the lower number of critical operations, the difference between the three implementations is not that much.



```{r plots}
library(ggplot2)
Nproc <- c(1,1,1,2,2,2,4,4,4,8,8,8,16,16,16,20,20,20)
ggplot(aes(y=Time, x = Nproc ,color=Implementation), data = data) + geom_line() +geom_point()


```

